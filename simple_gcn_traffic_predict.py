from pyspark.sql import SparkSession
from graphframes import GraphFrame
from graphframes.lib import AggregateMessages  # ç”¨äºé‚»å±…ç‰¹å¾èšåˆ
import numpy as np
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType
from pyspark.sql.functions import col, sum as spark_sum
from datetime import datetime, timedelta
import random

# åˆ›å»ºSparkä¼šè¯
spark = SparkSession.builder \
    .appName("SimpleGCNTrafficPrediction") \
    .getOrCreate()

# å®šä¹‰4ä¸ªèŠ‚ç‚¹çš„è·¯ç½‘ï¼ˆç¯å½¢ç»“æ„ï¼‰
vertices_data = [
    ("A", "Intersection A"),  # èŠ‚ç‚¹A
    ("B", "Intersection B"),  # èŠ‚ç‚¹B
    ("C", "Intersection C"),  # èŠ‚ç‚¹C
    ("D", "Intersection D")   # èŠ‚ç‚¹D
]

edges_data = [
    ("A", "B", "Road AB"),  # è¾¹Aâ†’B
    ("B", "C", "Road BC"),  # è¾¹Bâ†’C
    ("C", "D", "Road CD"),  # è¾¹Câ†’D
    ("D", "A", "Road DA")   # è¾¹Dâ†’Aï¼ˆå½¢æˆç¯å½¢ï¼‰
]

# åˆ›å»ºé¡¶ç‚¹DataFrame
vertices_schema = StructType([
    StructField("id", StringType(), False),
    StructField("description", StringType(), True)
])

vertices_df = spark.createDataFrame(vertices_data, schema=vertices_schema)

# åˆ›å»ºè¾¹DataFrameå¹¶æ·»åŠ äº¤é€šæµé‡æ•°æ®
edges_schema = StructType([
    StructField("src", StringType(), False),
    StructField("dst", StringType(), False),
    StructField("road_name", StringType(), True)
])

edges_df = spark.createDataFrame(edges_data, schema=edges_schema)

# ç”Ÿæˆ48å°æ—¶ï¼ˆ2å¤©ï¼‰çš„æ—¶é—´åºåˆ—äº¤é€šæµé‡æ•°æ®
base_flows = {"A": 100, "B": 150, "C": 200, "D": 120}  # ä¸åŒè¾¹çš„åŸºç¡€æµé‡
time_based_flows = []
start_time = datetime(2023, 1, 1, 0, 0, 0)

for hour in range(48):  # 48å°æ—¶
    current_time = start_time + timedelta(hours=hour)
    
    # åˆ›å»ºå…·æœ‰æ˜æ˜¾æ—¶é—´æ¨¡å¼çš„æµé‡ï¼ˆæ—©æ™šé«˜å³°ã€æ·±å¤œä½è°·ï¼‰
    if 7 <= current_time.hour <= 9 or 17 <= current_time.hour <= 19:  # é«˜å³°æ—¶æ®µ
        time_factor = 2.0
    elif 22 <= current_time.hour <= 23 or 0 <= current_time.hour <= 5:  # æ·±å¤œä½è°·
        time_factor = 0.5
    else:
        time_factor = 1.0  # æ­£å¸¸æ—¶æ®µ
    
    for src in ["A", "B", "C", "D"]:
        # è®¡ç®—éšæœºæ³¢åŠ¨ï¼ˆÂ±20%ï¼‰
        random_factor = 1.0 + (random.random() - 0.5) * 0.4
        
        # è®¡ç®—å½“å‰å°æ—¶çš„æµé‡
        flow = base_flows[src] * time_factor * random_factor
        
        time_based_flows.append([
            src,
            hour,
            current_time.strftime("%Y-%m-%d %H:%M:%S"),
            flow
        ])

# åˆ›å»ºæ—¶é—´ç‰¹å¾DataFrame
time_features_schema = StructType([
    StructField("src", StringType(), False),
    StructField("hour", IntegerType(), False),
    StructField("timestamp", StringType(), True),
    StructField("flow", DoubleType(), False)
])

time_features_df = spark.createDataFrame(time_based_flows, schema=time_features_schema)

# åˆ›å»ºåŸºç¡€å›¾ç»“æ„
graph = GraphFrame(vertices_df, edges_df)

# æ·»åŠ äº¤é€šæµé‡ç‰¹å¾åˆ°é¡¶ç‚¹ï¼ˆå–å¹³å‡å€¼ä½œä¸ºåˆå§‹ç‰¹å¾ï¼‰
features_df = time_features_df.groupBy("src").avg("flow").withColumnRenamed("avg(flow)", "features")
vertices_with_features_df = vertices_df.join(features_df, vertices_df.id == features_df.src, "inner") \
    .select(vertices_df["id"], "description", "features")

# é‡æ–°åˆ›å»ºå¸¦ç‰¹å¾çš„å›¾
graph_with_features = GraphFrame(vertices_with_features_df, edges_df)

# æ˜¾ç¤ºå›¾çš„åˆå§‹çŠ¶æ€
print("ğŸŒ åˆå§‹å›¾ç»“æ„:")
graph_with_features.vertices.show()
graph_with_features.edges.show()

# æ˜¾ç¤ºæ—¶é—´åºåˆ—æµé‡æ•°æ®
print("ğŸ“ˆ æ—¶é—´åºåˆ—æµé‡æ•°æ®:")
time_features_df.show()

# å®šä¹‰é‚»å±…èšåˆå‡½æ•°
def aggregate_neighbors(g):    
    # ä½¿ç”¨AggregateMessagesè¿›è¡Œé‚»å±…ç‰¹å¾èšåˆ
    agg = g.aggregateMessages(
        spark_sum(AggregateMessages.msg).alias("aggregatedMsg"),
        sendToDst=col("src.features")
    )
    
    # æ›´æ–°èŠ‚ç‚¹ç‰¹å¾ï¼ˆå½“å‰ç‰¹å¾ + èšåˆç‰¹å¾ï¼‰
    updated_vertices = g.vertices.join(agg, g.vertices.id == agg.id, "left_outer") \
        .select(
            g.vertices["id"],
            "description",
            (g.vertices["features"] + agg["aggregatedMsg"]).alias("features")
        )
    
    # åˆ›å»ºæ–°çš„å›¾ç»“æ„
    return GraphFrame(updated_vertices, g.edges)

# æ¨¡æ‹Ÿå¤šå±‚GCNçš„ä¿¡æ¯ä¼ æ’­
num_iterations = 3  # æ¨¡æ‹Ÿ3å±‚GCN
current_graph = graph_with_features

for i in range(num_iterations):
    print(f"ğŸ”„ ç¬¬ {i+1} å±‚GCNä¿¡æ¯ä¼ æ’­...")
    current_graph = aggregate_neighbors(current_graph)
    print(f"ğŸ“Š å½“å‰èŠ‚ç‚¹ç‰¹å¾:")
    current_graph.vertices.show()

# åœæ­¢Sparkä¼šè¯
spark.stop()
